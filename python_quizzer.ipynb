{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "618d6acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "from IPython.display import display, Markdown\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "35abfe92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n"
     ]
    }
   ],
   "source": [
    "# Setting up\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "\n",
    "# MODEL_q_generator = \"gpt-5-nano-2025-08-07\"\n",
    "MODEL_q_generator = \"o4-mini-2025-04-16\"\n",
    "MODEL_evaluator = \"o4-mini-2025-04-16\"\n",
    "\n",
    "openai_client = OpenAI()\n",
    "google_client = OpenAI(\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64218684",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_q_generator_message = \"\"\"\n",
    "You are a python quiz generator. Your task is to generate a python code quiz based on several parmeters:\n",
    "- Difficulty: Easy, Medium, Hard\n",
    "- Subject: e.g., Python, Data Science, Machine Learning, etc.\n",
    "- quiz_type: e.g., multiple choice, true/false, open question, Spot the Bug, etc.\n",
    "\n",
    "\n",
    "### Important\n",
    "- Ensure all newlines and quotes inside the JSON values are properly escaped so the JSON is valid.\n",
    "- Do not output markdown code blocks (like ```json). Return raw JSON only.\n",
    "- Question should be short and concise, such that the answer to is not more than 2-3 sentences.\n",
    "\n",
    "Note that the user was asked about these previous codes, so please don't repeat them:\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e0efae",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORE_TOPICS = [\n",
    "    \"String Slicing & Indexing\",\n",
    "    \"f-strings and Formatting\",\n",
    "    \"List Comprehensions\",\n",
    "    \"Dictionary Methods (.get, .items)\",\n",
    "    \"Set Operations (union, intersection)\",\n",
    "    \"Tuple Unpacking\",\n",
    "    \"While Loops & Break/Continue\",\n",
    "    \"For Loops with enumerate() and zip()\",\n",
    "    \"Function *args and **kwargs\",\n",
    "    \"Lambda Functions\",\n",
    "    \"Type Conversion (int vs str vs float)\",\n",
    "    \"Basic File I/O (open/read/write)\",\n",
    "    \"The 'in' operator keyword\",\n",
    "    \"Boolean Logic (and, or, not)\"\n",
    "]\n",
    "\n",
    "# DS_TOPICS = [\n",
    "#     \"Pandas: DataFrame Filtering (.loc vs .iloc)\",\n",
    "#     \"Pandas: GroupBy and Aggregation\",\n",
    "#     \"Pandas: Handling Missing Values (fillna, dropna)\",\n",
    "#     \"NumPy: Array Broadcasting\",\n",
    "#     \"NumPy: Vectorized Operations\",\n",
    "#     \"Matplotlib: Basic Plotting commands\",\n",
    "#     \"Datetime Module manipulation\",\n",
    "#     \"JSON parsing (json.loads/dumps)\",\n",
    "#     \"Random Module (choice, shuffle, randint)\",\n",
    "#     \"Regular Expressions (re module)\"\n",
    "# ]\n",
    "ADVANCED_TOPICS = [\n",
    "    \"Decorators (@wraps)\",\n",
    "    \"Generators and the 'yield' keyword\",\n",
    "    \"Context Managers ('with' statement)\",\n",
    "    \"Class Inheritance & super()\",\n",
    "    \"Dunder Methods (__init__, __str__, __len__)\",\n",
    "    \"Mutable vs Immutable types (pass-by-reference)\",\n",
    "    \"Default Mutable Arguments trap\",\n",
    "    \"Global vs Local Scope\",\n",
    "    \"Exception Handling (try/except/else/finally)\",\n",
    "    \"The 'is' vs '==' operator\",\n",
    "    \"Recursion\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e91219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def system_generator_message(history=None, system_prefix=system_q_generator_message):\n",
    "    if not history:\n",
    "        history = []\n",
    "    system_message = system_prefix + \"\\n\".join(history)\n",
    "\n",
    "    return system_message \n",
    "\n",
    "def get_topics(level):\n",
    "    if level == \"Easy\":\n",
    "        return CORE_TOPICS\n",
    "    elif level == \"Medium\":\n",
    "        return CORE_TOPICS\n",
    "    elif level == \"Hard\":\n",
    "        return ADVANCED_TOPICS + CORE_TOPICS\n",
    "    else:\n",
    "        return CORE_TOPICS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "12115b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_q_generator_schema = {\n",
    "    \"name\": \"python_quiz\",\n",
    "    \"strict\": True,  # Highly recommended for reliability\n",
    "    \"schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"question\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"A beautiful markdown text with the question regarding the subject (if code needed, include it as markdown code). If quiz_type is multiple choices, include here the choices with line breaks.\",\n",
    "            },\n",
    "            \"hint\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"a hint to help the user if he struggles.\"\n",
    "            },\n",
    "            \"answer\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The correct answer to the question.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\n",
    "            \"question\", \n",
    "            \"hint\", \n",
    "            \"answer\"],\n",
    "        \"additionalProperties\": False  # REQUIRED when strict is True\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f41365ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_input(subject=None, difficulty=None, quiz_type=None):\n",
    "    for param in [subject, difficulty, quiz_type]:\n",
    "        if not param:\n",
    "            param = f\"no specific {param} was given.\"\n",
    "            \n",
    "    return f\"\"\"\n",
    "    Please, generate a unique python code question based on the following parameters:\n",
    "    - Subject: {subject}\n",
    "    - Difficulty: {difficulty}\n",
    "    - quiz type: {quiz_type}\n",
    "\n",
    "    The question should not be too long, such that an answer to is not more than 2-3 sentences.\n",
    "    Be creative and unique, don't repeat codes and think about this random seed to encourage you to be creative: {random.random()}\n",
    "\n",
    "    You can use this bank of topics as a reference to choose the code to be tested (be sure to follow the requested subject):\n",
    "    {random.choices(get_topics(difficulty), k=4)}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d896c974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question(subject=None, difficulty=None, quiz_type=None, history=None, to_stream=False):\n",
    "    user_prompt = user_input(subject, difficulty, quiz_type)\n",
    "    \n",
    "    try:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_generator_message(history)},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "        \n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=MODEL_q_generator,\n",
    "            messages=messages,\n",
    "            response_format={\"type\": \"json_schema\", \"json_schema\": json_q_generator_schema},\n",
    "            # presence_penalty=0.6\n",
    "        )\n",
    "        \n",
    "        # Parse the JSON string into a Python Dictionary\n",
    "        result_content = response.choices[0].message.content\n",
    "        quest_dict = json.loads(result_content)\n",
    "\n",
    "        quest_dict['question'] = quest_dict['question'].replace('\\\\n', '\\n')\n",
    "        # quest_dict['code'] = quest_dict['code'].replace('\\\\n', '\\n')\n",
    "        return quest_dict\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3dfa1a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Python Quiz ‚Äî Sets, \"in\" and Boolean Logic üåü\n",
       "\n",
       "Consider the following code:\n",
       "\n",
       "```python\n",
       "s1 = {'sun', 'moon', 'star'}\n",
       "s2 = {'comet', 'star'}\n",
       "\n",
       "result = ('sun' in s1 and 'comet' in s2) or ('planet' in s1 and not ('star' in s2))\n",
       "print(result)\n",
       "```\n",
       "\n",
       "What does this print?\n",
       "\n",
       "Choices:\n",
       "A) True\n",
       "B) False\n",
       "C) Raises a KeyError\n",
       "D) None"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = generate_question(\"Python\", \"easy\", \"multiple choice\")\n",
    "display(Markdown(question['question']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a279e121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A) True'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a389f3b",
   "metadata": {},
   "source": [
    "# Set Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b8f5a594",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_evaluator_message = \"\"\"\n",
    "You are a python quiz evaluator. Your task is to evaluate a the user's answer according to the true answer provided.\n",
    "You are given the next parameters:\n",
    "- question: The question asked to the user.\n",
    "- true_answer: The correct answer to the question.\n",
    "- user_answer: The user's answer to the question.\n",
    "\n",
    "## important: if the answer seems like a choice in a multiple choices question, it is enough for the user to specify the number/letter of the choice.\n",
    "Your response should be a json object containing exactly these keys:\n",
    "{\n",
    "    \"verdict\": \"verdict keyword selected from: 'Correct', 'Wrong', 'Partial'\"\n",
    "    \"explanation\": \"A short markdown text with the explanation regarding the user's answer. Use evaluating phrases such as 'Good job!', 'Almost there!', etc. elaborate only if not correct.\"\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "json_evaluator_schema = {\n",
    "    \"name\": \"evaluator_json\",\n",
    "    \"strict\": True,  # Highly recommended for reliability\n",
    "    \"schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"verdict\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"verdict keyword selected from: 'Correct', 'Wrong', 'Partial'\"\n",
    "            },\n",
    "            \"explanation\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"A markdown text with the explanation regarding the user's answer. Use evaluating phrases such as 'Good job!', 'Almost there!', etc.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"verdict\", \"explanation\"],\n",
    "        \"additionalProperties\": False  # REQUIRED when strict is True\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "13ce4795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(question, true_answer, user_answer):\n",
    "    user_prompt = f\"\"\" Please evaluate my answer to the following question:\n",
    "    - question: {question}\n",
    "    - true_answer: {true_answer}\n",
    "    - user_answer: {user_answer}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_evaluator_message},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "        \n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=MODEL_evaluator,\n",
    "            messages=messages,\n",
    "            response_format={\"type\": \"json_schema\", \"json_schema\": json_evaluator_schema}\n",
    "        )\n",
    "        \n",
    "        # Parse the JSON string into a Python Dictionary\n",
    "        result_content = response.choices[0].message.content\n",
    "        parsed_data = json.loads(result_content)\n",
    "        \n",
    "        return parsed_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\", \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef3c92d",
   "metadata": {},
   "source": [
    "# Some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "11c57c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Dict normalization puzzle üß©\n",
       "\n",
       "Consider the following Python code:\n",
       "\n",
       "```python\n",
       "pairs = [('1', 10), (1, '20'), ('2', 30.0), (3.0, '40')]\n",
       "extra = {'2', 3, 4}\n",
       "# build a dict normalizing keys and values\n",
       "d = {int(float(k)): str(int(float(v))) for k, v in pairs}\n",
       "# now update with extras (note set semantics and type conversions)\n",
       "d.update({int(float(k)): f\"extra-{int(float(k))}\" for k in extra})\n",
       "print(d)\n",
       "```\n",
       "\n",
       "What is printed by this code? Choose one:\n",
       "\n",
       "A) {1: '10', 2: '30', 3: '40', 4: 'extra-4'}\n",
       "B) {1: '20', 2: 'extra-2', 3: 'extra-3', 4: 'extra-4'}\n",
       "C) {1: '20', 2: '30', 3: '40'}\n",
       "D) {1: '20', 2: '30', 3: 'extra-3', 4: 'extra-4'}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = generate_question(\"Python - dicts\", \"medium\", \"multiple choice\")\n",
    "display(Markdown(question['question']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d36830fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verdict': 'Correct',\n",
       " 'explanation': 'Good job! Your answer matches the expected output.'}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_answer = \"B\"\n",
    "evaluate(question['question'], question['answer'], my_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1b34aa80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verdict': 'Correct',\n",
       " 'explanation': 'Good job! Your answer matches the expected output.'}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_answer = \"{1: '20', 2: 'extra-2', 3: 'extra-3', 4: 'extra-4'}\"\n",
    "evaluate(question['question'], question['answer'], my_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6bd36ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verdict': 'Wrong',\n",
       " 'explanation': \"Not quite. The first comprehension actually produces {1: '20', 2: '30', 3: '40'} (because (1, '20') overwrites ('1', 10)), then updating with the extras replaces keys 2 and 3 and adds 4, resulting in {1: '20', 2: 'extra-2', 3: 'extra-3', 4: 'extra-4'}.\"}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_answer = \"A\"\n",
    "evaluate(question['question'], question['answer'], my_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4b7f7a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verdict': 'Wrong',\n",
       " 'explanation': \"Wrong. The initial dict comprehension produces key 1 with value '20' (not '10') because the pair (1,'20') overrides ('1',10). After updating with extras, the final dict is {1:'20', 2:'extra-2', 3:'extra-3', 4:'extra-4'}.\"}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_answer = \"{1: '10', 2: 'extra-2', 3: 'extra-3', 4: 'extra-4'}\"\n",
    "evaluate(question['question'], question['answer'], my_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7daf0fc",
   "metadata": {},
   "source": [
    "# Set Quiz Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "940a1516",
   "metadata": {},
   "outputs": [],
   "source": [
    "class quizzer:\n",
    "    def __init__(self):\n",
    "        self.history_questions = []\n",
    "        self.user_score = 0\n",
    "        self.total_score = 0\n",
    "        self.current_hint = None\n",
    "        self.current_question = None\n",
    "        self.current_answer = None\n",
    "        self.last_subject = None\n",
    "        self.last_difficulty = None\n",
    "        self.last_quiz_type = None\n",
    "\n",
    "        self.already_answered = False\n",
    "\n",
    "    def reset_quiz(self):\n",
    "        self.history_questions = []\n",
    "        self.user_score = 0\n",
    "        self.total_score = 0\n",
    "        self.current_hint = None\n",
    "        self.current_question = None\n",
    "        self.current_answer = None\n",
    "        self.last_subject = None\n",
    "        self.last_difficulty = None\n",
    "        self.last_quiz_type = None\n",
    "\n",
    "        self.already_answered = False\n",
    "    \n",
    "    def new_question(self, subject=None, difficulty=None, quiz_type=None):\n",
    "        self.already_answered = False\n",
    "        output = generate_question(subject, difficulty, quiz_type, self.history_questions)\n",
    "        self.current_question = output[\"question\"].replace(\"\\\\n\", \"\\n\")\n",
    "        self.history_questions.append(self.current_question)\n",
    "        self.current_hint = output[\"hint\"]\n",
    "        self.current_answer = output[\"answer\"]\n",
    "        return self.current_question\n",
    "    \n",
    "    def evaluate_user_answer(self, user_answer):\n",
    "        if self.already_answered:\n",
    "            return \"This question was already answered, please move to the next question.\", \"\"\n",
    "        self.already_answered = True\n",
    "        evaluation = evaluate(self.current_question,self.current_answer, user_answer)\n",
    "        verdict = evaluation[\"verdict\"]\n",
    "        explanation = evaluation[\"explanation\"]\n",
    "        self.total_score += 1\n",
    "\n",
    "        if verdict == \"Correct\":\n",
    "            self.user_score += 1\n",
    "        elif verdict == \"Partial\":\n",
    "            self.user_score += 0.5\n",
    "        else:\n",
    "            self.user_score += 0\n",
    "        return verdict, explanation\n",
    "    \n",
    "    def get_current_hint(self):\n",
    "        return self.current_hint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f2bd054a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OHaus\\AppData\\Local\\Temp\\ipykernel_18640\\4059327743.py:2: UserWarning: The parameters have been moved from the Blocks constructor to the launch() method in Gradio 6.0: theme. Please pass these parameters to launch() instead.\n",
      "  with gr.Blocks(theme=gr.themes.Soft()) as demo:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7875\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7875/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"# üêº Pandas Indexing Challenge üêæ\\n\\n```python\\nimport pandas as pd\\ndf = pd.DataFrame({'A': [5, 15, 25], 'B': [10, 20, 30]}, index=[0, 1, 2])\\n```\\n\\nDescribe the outputs of `df.loc[0:1]` vs `df.iloc[0:1]` and explain why they differ.\"]\n",
      "Keyboard interruption in main thread... closing server.\n"
     ]
    }
   ],
   "source": [
    "def create_app():\n",
    "    with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "        # State Management: Initialize the class for every unique user session\n",
    "        quiz_state = gr.State(quizzer())\n",
    "\n",
    "        gr.Markdown(\"# üß† AI Quiz Generator\")\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1):\n",
    "                # Inputs\n",
    "                inp_subject = gr.Textbox(label=\"1. Topic/Subject\", placeholder=\"e.g. Python Lists\")\n",
    "                inp_difficulty = gr.Dropdown(choices=[\"Easy\", \"Medium\", \"Hard\"], value=\"Medium\", label=\"2. Difficulty\")\n",
    "                inp_type = gr.Dropdown(choices=[\"Multiple Answers\", \"Open Question\", \"True/False\", \"Spot the Bug\"], value=\"Open Question\", label=\"3. Question Type\")\n",
    "                \n",
    "                btn_generate = gr.Button(\"Generate Question\", variant=\"primary\")\n",
    "                \n",
    "                # Stats Display\n",
    "                out_score = gr.Markdown(\"### Score: 0/0\")\n",
    "                btn_reset = gr.Button(\"Reset Quiz\", variant=\"stop\")\n",
    "\n",
    "            with gr.Column(scale=2):\n",
    "                # Question Area\n",
    "                out_question = gr.Markdown(\"### Question will appear here...\", min_height=190)\n",
    "                \n",
    "                # Hint Area\n",
    "                with gr.Accordion(\"Need help?\", open=False):\n",
    "                    btn_hint = gr.Button(\"Get Hint\", size=\"sm\")\n",
    "                    out_hint = gr.Markdown(\"\")\n",
    "\n",
    "                # Answer Area\n",
    "                gr.Markdown(\"---\")\n",
    "                inp_answer = gr.Textbox(label=\"4. Your Answer\", placeholder=\"Type here...\", lines=3)\n",
    "                btn_submit = gr.Button(\"Submit Answer\")\n",
    "                \n",
    "                # Evaluation Area\n",
    "                out_eval_verdict = gr.Markdown(\"\")\n",
    "                out_eval_explanation = gr.Markdown(\"\")\n",
    "\n",
    "        # --- EVENT FUNCTIONS ---\n",
    "        \n",
    "        def on_generate(quiz, subject, diff, q_type):\n",
    "            # Logic wrapper\n",
    "            q_text = quiz.new_question(subject, diff, q_type)\n",
    "            print(quiz.history_questions)\n",
    "            return (\n",
    "                quiz,                           # Update State\n",
    "                q_text,                         # Show Question\n",
    "                \"\",                             # Clear Hint\n",
    "                \"\",                             # Clear Answer box\n",
    "                \"\",                             # Clear Verdict\n",
    "                \"\"                              # Clear Explanation\n",
    "            )\n",
    "\n",
    "        def on_hint(quiz):\n",
    "            return quiz.get_current_hint()\n",
    "\n",
    "        def on_submit(quiz, answer):\n",
    "            verdict, explanation = quiz.evaluate_user_answer(answer)\n",
    "            score_text = f\"### Score: {quiz.user_score}/{quiz.total_score} ({quiz.user_score/quiz.total_score*100}%)\"\n",
    "            \n",
    "            # Formatting verdict color\n",
    "            if verdict == \"Correct\":\n",
    "                verdict_md = f\"### ‚úÖ {verdict}\"\n",
    "            elif verdict == \"Answered\":\n",
    "                verdict_md = f\"### ‚ö†Ô∏è {verdict}\"\n",
    "            else:\n",
    "                verdict_md = f\"### ‚ùå {verdict}\"\n",
    "            \n",
    "            return quiz, verdict_md, explanation, score_text\n",
    "\n",
    "        def on_reset(quiz):\n",
    "            quiz.reset_quiz()\n",
    "            return (\n",
    "                quiz, \n",
    "                \"### Score: 0/0\", \n",
    "                \"### Question will appear here...\", \n",
    "                \"\", \n",
    "                \"\", \n",
    "                \"\", \n",
    "                \"\"\n",
    "            )\n",
    "\n",
    "        # --- WIRING BUTTONS ---\n",
    "        \n",
    "        btn_generate.click(\n",
    "            on_generate, \n",
    "            inputs=[quiz_state, inp_subject, inp_difficulty, inp_type], \n",
    "            outputs=[quiz_state, out_question, out_hint, inp_answer, out_eval_verdict, out_eval_explanation]\n",
    "        )\n",
    "\n",
    "        btn_hint.click(\n",
    "            on_hint,\n",
    "            inputs=[quiz_state],\n",
    "            outputs=[out_hint]\n",
    "        )\n",
    "\n",
    "        btn_submit.click(\n",
    "            on_submit,\n",
    "            inputs=[quiz_state, inp_answer],\n",
    "            outputs=[quiz_state, out_eval_verdict, out_eval_explanation, out_score]\n",
    "        )\n",
    "\n",
    "        btn_reset.click(\n",
    "            on_reset,\n",
    "            inputs=[quiz_state],\n",
    "            outputs=[quiz_state, out_score, out_question, out_hint, inp_answer, out_eval_verdict, out_eval_explanation]\n",
    "        )\n",
    "\n",
    "    return demo\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = create_app()\n",
    "    app.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a2b23c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_Gardio_Apps (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
